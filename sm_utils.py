import random
import torch

from tqdm import tqdm


def train(
    model, dataset, loss_fn, n_epochs=5000, lr=3e-4, batch_size=32, prior_score=False
):
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    dloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    with tqdm(range(n_epochs), desc="Training epochs") as tepoch:
        for _ in tepoch:
            total_loss = 0
            for data in dloader:
                # get batch data
                if len(data) > 1:
                    theta, x, kwargs_sn = get_batch_data(data, prior_score)
                    kwargs_sn["x"] = x
                else:
                    theta = data[0]
                    kwargs_sn = {}
                # train step
                opt.zero_grad()
                loss = loss_fn(theta, **kwargs_sn)
                loss.backward()
                opt.step()

                # running stats
                total_loss = total_loss + loss.detach().item() * theta.shape[0]

            tepoch.set_postfix(loss=total_loss / len(dataset))


# Training with validation and early stopping as in
# https://github.com/smsharma/mining-for-substructure-lens/blob/master/inference/trainer.py


def train_with_validation(
    model,
    dataset,
    loss_fn,
    n_epochs=100,
    lr=1e-4,
    batch_size=128,
    lr_decay=1e-2,
    lr_update_freq=200,
    validation_split=0.25,
    early_stopping=False,
    patience=1000,
    prior_score=False,
    save_path=None,
    losses_filename="losses.pkl",
    model_filename="score_network.pkl",
):
    # get train and validation loaders
    train_loader, val_loader = get_dataloaders(
        dataset, batch_size=batch_size, validation_split=validation_split
    )

    # set up optimizer
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    # start training
    train_losses, val_losses = [], []
    best_loss, best_model, best_epoch = None, None, None

    with tqdm(range(n_epochs), desc="Training epochs") as tepoch:
        for e in tepoch:
            # update learning rate
            # lr_e = update_lr(lr, lr_decay, e, n_epochs)
            if (e + 1) % lr_update_freq == 0:
                lr = lr * lr_decay
                set_lr(opt, lr)

            # training loop
            train_loss = 0
            for data in train_loader:
                # get batch data
                theta, x, kwargs_sn = get_batch_data(data, prior_score)

                # training step
                opt.zero_grad()
                loss = loss_fn(theta, x, **kwargs_sn)
                loss.backward()
                opt.step()

                # update loss
                train_loss += loss.detach().item() * theta.shape[0]  # unnormalized loss
            train_loss /= len(dataset) * (1 - validation_split)  # normalized loss
            train_losses.append(train_loss)

            # validation loop
            if val_loader is not None:
                with torch.no_grad():
                    val_loss = 0
                    for data in val_loader:
                        # get batch data
                        theta, x, kwargs_sn = get_batch_data(data, prior_score)

                        # validation step
                        loss = loss_fn(theta, x, **kwargs_sn)

                        # update loss
                        val_loss += (
                            loss.detach().item() * theta.shape[0]
                        )  # unnormalized loss
                    val_loss /= len(dataset) * validation_split  # normalized loss
                    val_losses.append(val_loss)

            tepoch.set_postfix(loss=train_loss, lr=lr)

            # early stopping
            if early_stopping:
                assert (
                    validation_split > 0
                ), "validation data is required for early stopping"
                if best_loss is None or val_loss < best_loss:
                    best_loss = val_loss
                    best_model = model
                    best_epoch = e
                elif e - best_epoch > patience:
                    break

        if early_stopping:
            model = best_model
            e = best_epoch
            print(
                f"EARLY STOPPING: best validation loss {best_loss} at epoch {best_epoch}"
            )
            print("Did not improve for {} epochs".format(min(patience, n_epochs - e)))

        # save losses and best model
        if save_path is not None:
            torch.save(
                {
                    "train_losses": train_losses,
                    "val_losses": val_losses,
                    "best_epoch": e,
                },
                save_path + losses_filename,
            )
            torch.save(model, save_path + model_filename)

    return model


def get_dataloaders(dataset, batch_size, validation_split):
    if validation_split > 0:
        dataset_train, dataset_val = torch.utils.data.random_split(
            dataset, [1 - validation_split, validation_split]
        )
        train_loader = torch.utils.data.DataLoader(
            dataset_train, batch_size=batch_size, shuffle=True
        )
        val_loader = torch.utils.data.DataLoader(
            dataset_val, batch_size=batch_size, shuffle=True
        )
    else:
        dataset_train = dataset
        train_loader = torch.utils.data.DataLoader(
            dataset_train, batch_size=batch_size, shuffle=True
        )
        val_loader = None
    return train_loader, val_loader


def update_lr(lr, lr_decay, epoch, n_epochs):
    return lr * lr_decay ** (epoch / (n_epochs - 1))


def set_lr(optimizer, lr):
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


def get_batch_data(data, prior_score):
    # default: npse without n
    theta, x = data[0], data[1]
    n = None
    kwargs_sn = {}

    if len(data) > 2:
        n = data[2]
        kwargs_sn = {"n": n}

    # learn diffused prior score: 20% of the time, set context to zero
    if prior_score and random.random() < 0.2:
        x = torch.zeros_like(x)  # zero context
        kwargs_sn = {}
        if n is not None:
            n = torch.zeros((x.shape[0], 1))  # zero set size
            kwargs_sn["n"] = n

    return theta, x, kwargs_sn
